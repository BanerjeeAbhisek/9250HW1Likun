---
title: "Homework 1"
author: "Abhisek Banerjee"
date: "2026-02-08"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1

## a)

We have a matrix $\Sigma = \sigma I + KK'$ where $K$ is an $N \times M$ matrix having $i.i.d$ $\mathcal{N}(0,1)$ random variables in its element. As given in question, we take $\sigma=0.2$ and fix $M$ at 10. 

```{r}
M = 10
sig = 0.2
```

Looking at my computer condition, I will go with the following grid for $N$.

```{r}
ns = c(50, 100, 200, 500, 1000, 2000, 3000, 5000)
```


We set a seed for reproducibility

```{r}
set.seed(12345678)
```

Two time points storing variable to note the time for the two methods.

```{r}
t1 = rep(0, length(ns))
t2 = rep(0, length(ns))
```

From slide 25 we get the Sherman-Morrison Woodbury formula as 

\[
(A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}
\]

For our problem, $\Sigma = \sigma I + KK^\top$, we note:
\[
A = \sigma I, \quad U = K, \quad C = I_M, \quad V = K^\top
\]

This gives us:
\[
A^{-1} = \frac{1}{\sigma}I, \quad C^{-1} = I_M, \quad VA^{-1}U = \frac{1}{\sigma}K^\top K
\]

Substituting into the SMW formula:
\[
\Sigma^{-1} = \frac{1}{\sigma}I - \frac{1}{\sigma}I \cdot K\left(I_M + \frac{1}{\sigma}K^\top K\right)^{-1}K^\top \cdot \frac{1}{\sigma}I
\]

Simplifying the above we obtain:
\[
\Sigma^{-1} = \frac{1}{\sigma}I - \frac{1}{\sigma^2}K\left(I_M + \frac{1}{\sigma}K^\top K\right)^{-1}K^\top
\]


Now,we do matrix inversion wuth method 1 and 2:

```{r}
for (i in 1:length(ns)) {
  n = ns[i]
  K = matrix(rnorm(n * M), nrow = n, ncol = M)
  S = sig * diag(n) + K %*% t(K)
  t1[i] = system.time(solve(S))[3]
  t2[i] = system.time({
    Ainv = (1/sig) * diag(n)            
    Cinv = diag(M)                     
    VAU = t(K) %*% Ainv %*% K           
    mid = solve(Cinv + VAU)             
    ans = Ainv - Ainv %*% K %*% mid %*% t(K) %*% Ainv
  })[3]
  
  cat("n =", n, "| solve:", t1[i], "| smw:", t2[i], "\n")
}

```



Now we make the plot asked in the question:

```{r}
plot(ns, t1, type = "b", col = "red", pch = 16,
     xlab = "N", ylab = "time (s)",
     main = "CPU Time vs N",
     ylim = range(c(t1, t2)))
lines(ns, t2, type = "b", col = "blue", pch = 17)
legend("topleft", legend = c("solve()", "SMW"),
       col = c("red", "blue"), pch = c(16, 17), lty = 1)

```


## b)

From the pdf on FLOPS uploaded in canvas, "flops of different operations" we calculate the flops for the Sherman Woodbury algorithm as follows:

We compute $\Sigma^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}$ where $A = \sigma I$, $U = K$, $C = I_M$, $V = K^\top$.

- $A^{-1} = \frac{1}{\sigma}I$: $N$ flops (diagonal inverse)
- $VA^{-1} = K^\top \cdot \frac{1}{\sigma}I$: $NM$ flops (scale each entry)
- $VA^{-1}U = (K^\top)(K)$, $(M \times N)(N \times M)$: $2NM^2$ flops
- $C^{-1} + VA^{-1}U$, $M \times M$ addition: $M^2$ flops
- Solve $(C^{-1} + VA^{-1}U)^{-1}(VA^{-1})$, i.e. $M \times M$ system applied to $M \times N$ matrix: $M^3 + 2M^2 N$ flops
- $A^{-1}U = \frac{1}{\sigma}K$: $NM$ flops
- $(A^{-1}U) \cdot \text{mid result}$, $(N \times M)(M \times N)$: $2N^2 M$ flops
- Subtract from $A^{-1}$: $N^2$ flops

Total: $N + 2NM + 2NM^2 + M^2 + M^3 + 2M^2N + 2N^2M + N^2$. With $M = 10$ fixed, the dominant term is $2N^2 M = 20N^2$, so the cost is $O(N^2)$.

For the the algorithm with `solve()` function, we solve $N \times N$ giving us $N^3$. The following code in R gives the plot:

```{r}
f1 = ns^3
f2 = ns + 2*ns*M + 2*ns*M^2 + M^2 + M^3 + 2*M^2*ns + 2*ns^2*M + ns^2
plot(ns, f1, type = "b", col = "red", pch = 16, log = "y",
     xlab = "N", ylab = "Flops",
     main = "Floating Point Operations vs N",
     ylim = range(c(f1, f2)))
lines(ns, f2, type = "b", col = "blue", pch = 17)
legend("topleft", legend = c("solve()", "SMW"),
       col = c("red", "blue"), pch = c(16, 17), lty = 1)
```

## 1c

Looking at the two plots, it seems like, for small $N$, for example untill 2000 the difference in computation time by the two methods is nit that  significant. However as $N$ increases, the difference in computation time increases for both and the difference in time between the two algorithms starts to increase more and more. A similar observation can be seen for the second graph as well, increasing $N$ leads to a steady increase in floating point operations in both, however for the solve function, just like computation time, the floating point operations are way more than the SMW algorithm. This shows that algebric manipulation can significantly reduce computation time and can beat already optimized inbuilt codes in programming languages.   

# Problem 2

We have an already developed a spatio-temporal extreme-value model to estimate the theoretical upper bound of temperature at a given location from historical records at weather stations. The following code shows confirms the dimension of the covariate matrix, the outcome vector and the structure of the weather stations data from the file `Temp_data.RData. In the question it is said to contain a global dataset of temperature upper bounds evaluated at $n = 37511$ weather stations

```{r}
load("Temp_data.RData")
dim(Xmat)
length(Temp_UB)
head(stations)
str(stations)
summary(Temp_UB)
```

Here we have:
- $y$ as vector of `Temp_UB`
- $X$ as the matrix `Xmat` with $p=3535$ columns. 
- `stations` containing latitutde and longitude of the stations. 

Goal is to approximate high-dim `Temp_UB` field via a basis expansion $$\mathbf{y} \approx \mathbf{X}\boldsymbol{\beta},$$
where $\beta$ being the coefficients. 

### 1)

I randomly chose two basis functions, $25$ and $386$ and plotted them below. I saw that it is kind of difficult to view with legends being on side of the plots, so I modified the code to put the legends below the plots. 



```{r}
library(gridExtra)
library(ggplot2)
library(maps)

p1 = ggplot(stations) +
  geom_point(aes(x = lon, y = lat, col = Xmat[, 25]), size = 0.7) +
  geom_path(
    data = map_data("world"),
    aes(x = long, y = lat, group = group),
    color = "gray", linewidth = 0.2, alpha = 0.5
  ) +
  scale_color_gradientn(
    colours = terrain.colors(10),
    name = "Basis #25",
    na.value = "transparent",
    limits = c(0, 0.15)
  ) +
  theme(
    legend.position = "bottom",
    legend.direction = "horizontal"
  )

p2 = ggplot(stations) +
  geom_point(aes(x = lon, y = lat, col = Xmat[, 386]), size = 0.7) +
  geom_path(
    data = map_data("world"),
    aes(x = long, y = lat, group = group),
    color = "gray", linewidth = 0.2, alpha = 0.5
  ) +
  scale_color_gradientn(
    colours = terrain.colors(10),
    name = "Basis #386",
    na.value = "transparent",
    limits = c(0, 0.15)
  ) +
  theme(
    legend.position = "bottom",
    legend.direction = "horizontal"
  )

grid.arrange(p1, p2, ncol = 2)
```
We see from the left plot that basis #25 correspond to the localized bumps near western Alaska and  basis #386 correspond to the localized bumps at southern Canada above the boundary between United States and Canada. 

## 2)

In our context, a local basis function is a column from the matrix `Xmat` which can be used to obtain a bump in a small spatial region. With respect to the figure given in the question, we see that a local basis function #1 activates in a small geographic region and can be seen as a bump near southern Sweden. Similarly from part 1 we see that a basis of 100 leads to a bump in Alaska. In this context, it is also noted that a local basis function is only active in a particular small geographic region and is almost zero everywhere else, which can be seen with green shade. 


## 3) 

### a)

We run `solve(t(Xmat)%*%Xmat)` in the following code:

```{r error=TRUE}
solve(t(Xmat)%*%Xmat)
```

Upon running the above code, I get the following error, "Error in solve.default(t(Xmat) %*% Xmat) : Lapack routine dgesv: system is exactly singular: U[1362,1362] = 0". This error tells us that $X'X$ is singular here. From Linear Models course we know that a system of linear equation is singular or $(X'X)^{-1}$ doesn't exists if $X$ is not of full column rank. This means all the column vectors in $X$ are not linearly independent. In other words, at least one  column vector can be written as a linear combinition of the others. Here, it simply means `Xmat` has some (at least one) linearly dependent column. We know from part 1, the columns of `Xnew` are localized basis functions. A good reason for very similar columns or local basis functions can occur due to overlapping or being near identical making some redundent. Numerical singualr means that theoretically it is non singular but the values of some colunns maybe so similar that it can lead to extremely small singular values, so small small that R makes it 0. 


### b) 

We consider the SVD of $X$ as  
\[
X_{n \times p} = U_{n \times p}\, D_{p \times p}\, V_{p \times p}^{\top}.
\]
Here the columns of $U$ are the eigenarrays. $D$ has the singular values. It is a diagonal matrix with diagonal elements as $\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_p \ge 0$. We get a plot of the singular values of $X$ as:

```{r}
s <- svd(Xmat, nu=0, nv=0)$d
plot(s, type="b", pch=20, xlab="index", ylab="singular
            value")
plot(log(s), type="b", pch=20, xlab="index",
            ylab="singular value in log scale")
```

From the first plot we see that the singular values of `Xmat` decays very rapidly, we notice that from around 2000 the singular values almost touches 0 sugessting extremely small singular values. Thus we have a very log tail with almost 0 singular values. In the second graph, taking log of singular values make it linear and easy to visualize the small singular values. Since its hard to distinguish between to tiny numbers, for example of order $10^{-9}$, taling log makes it easy to view as log makes it around -20. We see from the second plot that there are singular values whose log is around -40, this means ther are of order $\exp (-40)$. This is extremeply small. Considering $U$ and $V$ to be orthogonal matrices, now we have  \[
X = U D V^\top,
\qquad
D = \mathrm{diag}(\sigma_1,\ldots,\sigma_p),
\quad
\sigma_1 \ge \cdots \ge \sigma_p > 0 .
\]
Now, if we calculate $X^\top X$, we get:
\[
X^\top X
=
(VD U^\top)^\top (UD V^\top)
=
VD^\top D V^\top
=
V\,\mathrm{diag}(\sigma_1^2,\ldots,\sigma_p^2)\,V^\top.
\]
Now ,
\[
(X^\top X)^{-1}
=
V\,\mathrm{diag}\!\left(\frac{1}{\sigma_1^2},\ldots,\frac{1}{\sigma_r^2}\right)V^\top.
\]

Here, if $\sigma_i$ for different $i$ in $1,..,p$ are very small, almost zero, this makes $\frac{1}{\sigma_1}$ almost infinity causing instability in inverse problems. 

## 4)

Using the `svd()` function in R we can compute $V,\Sigma^+$ and $U$. 

```{r}
s = svd(Xmat)       
d = s$d
```

We have y as our `Temp_UB`. Now we plug-in these in the formula $\hat{\beta}_{\text{svd}}= V\Sigma^+U^\top y.$ Herewe print the first five values of $\hat{\beta}_{\text{svd}}$   

```{r}
y= Temp_UB
b = s$v %*% ( (t(s$u) %*% y) / d )
head(b)
```

We know $\hat y = X \hat{\beta}_{\text{svd}}$, Next we plot using the code already given in (1).
   
```{r}


# reconstruction: yhat = X b
yh = as.vector(Xmat %*% b)

# plot like the top-right panel (reconstruction)
p = ggplot(stations) +
  geom_point(aes(x = lon, y = lat, col = yh), size = 0.7) +
  geom_path(data = map_data("world"),
            aes(x = long, y = lat, group = group),
            color = "gray", linewidth = 0.2, alpha = 0.5) +
  scale_color_gradientn(colours = terrain.colors(10), name = "yh",
                        na.value = "transparent") +
  coord_fixed() +
  theme_bw()

p
```

It looks from the above plot that, the SVD approach was able to capture the spatial structure of the temperature upper bound for most of the areas. In the Great Plains region of U.S., since it is a relatively homogeneous area with similar altitude everywhere, the reconstruction by SVD appears reasonable as temperature variation is smooth and aligned with localized basis functions we saw in Figure 1. However, in the Rocky Mountain regions, where altitude varies and temperature fluctuates rapidly from region to region, the reconstruction quality seems to be weaker and deviates from top left figure in Figure 1.  

From Linear models class we know, $X \hat{\beta}_{\text{svd}}$ is the orthogonal projection of $y$ onto the span of $X$. Since the small-scale noise from measurement error are not represented by this basis, these are automatically filtered out in this approach.


### b)

The following figure shows the histograms of $\hat{\beta}_{\text{svd}}$ and residuals. 


```{r}
r = as.vector(y - yh)
par(mfrow=c(1,2))
hist(b, breaks=60, main="Histogram of beta_svd", xlab="beta_svd", col="gray")
hist(r, breaks=60, main="Histogram of residuals", xlab="r = y - X beta_svd", col="gray")
par(mfrow=c(1,1))
```


### c)

We say that $$\hat\beta_{\mathrm{svd}} = V\Sigma^{+}U^\top y,
\qquad
\Sigma^{+}=\mathrm{diag}\!\left(\frac{1}{\sigma_i}\right),$$

Here, if the values of $\sigma_i$ are very small then $\frac{1}{\sigma_i}$ will have very very large values, these crazy large values, when multiplied with the rest in $V\Sigma^{+}U^\top y$ makes $\hat\beta_{\mathrm{svd}}$ to almost explode and we just saw it became as large as $1 \exp(17)$. But $X \hat\beta_{\mathrm{svd}}$ on the other hand does not show that crazy high values, which makes the residuals lie almost around $-20$ and $10$. This happens because, for small $\sigma_i$ values, these coefficients contribute little to $X \hat\beta_{\mathrm{svd}}$. These coefficients contribute near the nullspace of X, hence when multiplied with $X$, it does not contribute much to $\hat y$ and their magnifying effect is not seen. 

## 5) 

Now we do truncated SVD. That is, if $\sigma_i$ is below certain threshold $\tau$, then set $\frac{1}{\sigma_i}=0.$ Now we define \[
\hat{\beta}_{\tau}
=
V \, \mathrm{diag}\!\left(
\frac{1}{\sigma_i}\,\mathbf{1}\{\sigma_i \ge \tau\}
\right) U^{\top} y.
\]

### a)

The truncated SVD which we implemented is present in the code below. It shows the first five values of $\hat{\beta}_{\tau}$.
```{r}
tau = 1e-10
keep = d >= tau
bt = s$v[, keep, drop=FALSE] %*% ((t(s$u[, keep, drop=FALSE]) %*% y) / d[keep])
head(bt)
```

When we see the second plot which is for the singular values in log scale in 3 b, we note that after $10^{-10}$ the drop is huge. Until that value,we cover a large part of the graph essentially giving us a major portion of singular values. Thus if we choose $10^{-10}$, we do not loose most of the singular values yet drop the extremely small ones, hence we choose $tau = 10^{-10}$.

### b)

Now we compare the histograms of coefficients before and after truncation, that is $\hat{\beta}_{\text{svd}}$ and $\hat{\beta}_{\tau}$.

```{r}
r = as.vector(y - yh)
par(mfrow=c(1,2))
hist(b, breaks=60, main="Histogram of beta_svd", xlab="beta_svd", col="gray")
hist(bt, breaks=60, main="Histogram of beta_tau", xlab="beta_tau", col="gray")
par(mfrow=c(1,1))
```


Now we see that, after truncating the range of $\hat{\beta}$ shrinked to around $-400$ to $600$ instead of a swing from $=4 \exp(16)$ to $1 \exp(17)$. This remarkable reduction in the range of $\hat{\beta}$ proves that the truncated SVD procedure we implementedis very effective.


### c)

Now we produce another rescontruction figure using $X \hat{\beta}_\tau$.

```{r}
yht_new = as.vector(Xmat %*% bt)
p_tau = ggplot(stations) +
  geom_point(aes(x = lon, y = lat, col = yht_new), size = 0.7) +
  geom_path(
    data = map_data("world"),
    aes(x = long, y = lat, group = group),
    color = "gray", linewidth = 0.2, alpha = 0.5
  ) +
  scale_color_gradientn(
    colours = terrain.colors(10),
    name = "X %*% beta_tau",
    na.value = "transparent"
  ) +
  coord_fixed() +
  theme_bw()

p_tau
```

## 6)

We consider ridge (Tikhonov) regularization
$$
\hat{\beta}_\lambda=\arg\min_{\beta}\|y - X\beta\|_2^2 + \lambda \|\beta\|_2^2.
$$

Now we take derivatives with respect to $\beta$ and set them to $0$ yeilding us:
$$
-2X^\top(y - X\beta) + 2\lambda \beta = 0,
$$
we can rewrite this as:
$$
(X^\top X + \lambda I)\beta = X^\top y.
$$

Therefore, 
$$
\hat{\beta}_\lambda = (X^\top X + \lambda I)^{-1} X^\top y.
$$

Now let the singular value decomposition of $X$ be
$$
X = U \Sigma V^\top,
$$
where $\Sigma = \mathrm{diag}(\sigma_1,\ldots,\sigma_r)$.
Then
$$
X^\top X = V \Sigma^\top \Sigma V^\top
= V \, \mathrm{diag}(\sigma_1^2,\ldots,\sigma_r^2) \, V^\top,
$$
and
$$
X^\top y = V \Sigma^\top U^\top y
= V \, \mathrm{diag}(\sigma_1,\ldots,\sigma_r) \, U^\top y.
$$

Hence we get,
$$
X^\top X + \lambda I
= V (\Sigma^\top \Sigma + \lambda I) V^\top,
$$
so
$$
(X^\top X + \lambda I)^{-1}
= V (\Sigma^\top \Sigma + \lambda I)^{-1} V^\top.
$$

Substituting back into the expression for $\hat{\beta}_\lambda$ yields us
$$
\hat{\beta}_\lambda
=
V (\Sigma^\top \Sigma + \lambda I)^{-1} \Sigma^\top U^\top y.
$$

Now we know that $\Sigma^\top \Sigma = \mathrm{diag}(\sigma_i^2)$, thus we have
$$
(\Sigma^\top \Sigma + \lambda I)^{-1} \Sigma^\top
=
\mathrm{diag}\!\left(
\frac{\sigma_i}{\sigma_i^2 + \lambda}
\right).
$$

Thus we can rewrite the ridge estimator as:
$$
\boxed{
\hat{\beta}_\lambda
=
V \, \mathrm{diag}\!\left(
\frac{\sigma_i}{\sigma_i^2 + \lambda}
\right) U^\top y.
}
$$

Now we implement this in R. We consider four different types of $\lambda$ values as $1e-4, 1e-2, 1, 10$.

```{r}
u  = s$u
v  = s$v
rmse_t = sqrt(mean((y - yht_new)^2, na.rm=TRUE))
lam = c(1e-4, 1e-2, 1, 10)
out = data.frame(lambda=lam, rmse=NA, coef_l2=NA)
for (i in 1:length(lam)) {
  l = lam[i]
  br = v %*% ((d / (d^2 + l)) * (t(u) %*% y))
  yhr = as.vector(Xmat %*% br)
  out$rmse[i] = sqrt(mean((y - yhr)^2, na.rm=TRUE))
  out$coef_l2[i] = sqrt(sum(br^2))
}
```

The table below shows the RMSE obtained by this method for different choices of $\lambda=1e-4$ which corresponds to the smallest RMSE for different $\lambda$ choices.

```{r}
out
```

The RMSE from the truncated SVD model is:

```{r}
rmse_t
```
We note that for all our choices of $\lambda$, we smaller RMSE for the truncaed SVD procedure. 

To compare coefficient magnitude, we plot the histograms. We take the case for $\lambda=1e-04$

```{r}
l0 = lam[1]
br0 = v %*% ((d / (d^2 + l0)) * (t(u) %*% y))
par(mfrow=c(1,2))
hist(bt, breaks=60, main="Histogram of beta_tau", xlab="beta_svd", col="gray")
hist(br0, breaks=60, main="Histogram of beta_lambda", xlab="beta_lambda", col="gray")
par(mfrow=c(1,1))
```


Here we see that the coefficients magnitude range has shrunk further for the rigge procedure than the truncated SVD procedure but this led to a very slight increase in RMSE. 


## Problem 3

```{r warning=FALSE}
## =============================================================
## Problem 3: Simulation Study - MLE vs MoM for Gamma(alpha, beta)
## True: alpha = 3, beta = 7, so E[X] = alpha*beta = 21
## =============================================================

library(MASS)  # needed for fitdistr()

set.seed(123)  # for reproducibility

# --- True parameters ---
alpha_true <- 3
beta_true  <- 7

# --- Simulation settings ---
n_grid <- c(20, 50, 100, 200, 500, 1000)  # sample sizes to try
B      <- 1000                              # number of simulations per n

# --- Storage vectors ---
mle_bias_alpha <- mle_bias_beta <- numeric(length(n_grid))
mle_mse_alpha  <- mle_mse_beta  <- numeric(length(n_grid))
mom_bias_alpha <- mom_bias_beta <- numeric(length(n_grid))
mom_mse_alpha  <- mom_mse_beta  <- numeric(length(n_grid))

# --- Main simulation loop ---
for (i in seq_along(n_grid)) {
  n <- n_grid[i]
  
  # Vectors to hold B estimates for this sample size
  mle_a <- mle_b <- numeric(B)
  mom_a <- mom_b <- numeric(B)
  
  for (j in 1:B) {
    # Step 1: Generate a random sample from Gamma(shape=3, scale=7)
    x <- rgamma(n, shape = alpha_true, scale = beta_true)
    
    # Step 2: Method of Moments estimators
    # For Gamma: E[X] = alpha*beta, Var[X] = alpha*beta^2
    # Solving: alpha_hat = xbar^2 / s^2,  beta_hat = s^2 / xbar
    xbar <- mean(x)
    s2   <- var(x)
    mom_a[j] <- xbar^2 / s2   # alpha hat (MoM)
    mom_b[j] <- s2 / xbar     # beta hat (MoM)
    
    # Step 3: MLE using fitdistr
    # Note: fitdistr fits shape and RATE (not scale)
    # So beta_hat_MLE = 1 / rate_hat
    fit <- fitdistr(x, "gamma")
    mle_a[j] <- fit$estimate["shape"]
    mle_b[j] <- 1 / fit$estimate["rate"]  # convert rate to scale
  }
  
  # Step 4: Compute Bias = mean(estimates) - true value
  mle_bias_alpha[i] <- mean(mle_a) - alpha_true
  mle_bias_beta[i]  <- mean(mle_b) - beta_true
  mom_bias_alpha[i] <- mean(mom_a) - alpha_true
  mom_bias_beta[i]  <- mean(mom_b) - beta_true
  
  # Step 5: Compute MSE = mean((estimates - true value)^2)
  mle_mse_alpha[i] <- mean((mle_a - alpha_true)^2)
  mle_mse_beta[i]  <- mean((mle_b - beta_true)^2)
  mom_mse_alpha[i] <- mean((mom_a - alpha_true)^2)
  mom_mse_beta[i]  <- mean((mom_b - beta_true)^2)
}

# --- Print results table ---
results <- data.frame(
  n = n_grid,
  MLE_Bias_alpha = round(mle_bias_alpha, 4),
  MoM_Bias_alpha = round(mom_bias_alpha, 4),
  MLE_MSE_alpha  = round(mle_mse_alpha, 4),
  MoM_MSE_alpha  = round(mom_mse_alpha, 4),
  MLE_Bias_beta  = round(mle_bias_beta, 4),
  MoM_Bias_beta  = round(mom_bias_beta, 4),
  MLE_MSE_beta   = round(mle_mse_beta, 4),
  MoM_MSE_beta   = round(mom_mse_beta, 4)
)
print(results)

# --- Plotting (2x2 panel) ---
par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))

# Plot 1: Bias of alpha
plot(n_grid, mle_bias_alpha, type = "b", col = "blue", pch = 16,
     xlab = "Sample size (n)", ylab = "Bias", main = "Bias of alpha hat")
lines(n_grid, mom_bias_alpha, type = "b", col = "red", pch = 17)
abline(h = 0, lty = 2)
legend("topright", legend = c("MLE", "MoM"), col = c("blue", "red"),
       pch = c(16, 17), lty = 1, cex = 0.8)

# Plot 2: Bias of beta
plot(n_grid, mle_bias_beta, type = "b", col = "blue", pch = 16,
     xlab = "Sample size (n)", ylab = "Bias", main = "Bias of beta hat")
lines(n_grid, mom_bias_beta, type = "b", col = "red", pch = 17)
abline(h = 0, lty = 2)
legend("topright", legend = c("MLE", "MoM"), col = c("blue", "red"),
       pch = c(16, 17), lty = 1, cex = 0.8)

# Plot 3: MSE of alpha
plot(n_grid, mle_mse_alpha, type = "b", col = "blue", pch = 16,
     xlab = "Sample size (n)", ylab = "MSE", main = "MSE of alpha hat")
lines(n_grid, mom_mse_alpha, type = "b", col = "red", pch = 17)
legend("topright", legend = c("MLE", "MoM"), col = c("blue", "red"),
       pch = c(16, 17), lty = 1, cex = 0.8)

# Plot 4: MSE of beta
plot(n_grid, mle_mse_beta, type = "b", col = "blue", pch = 16,
     xlab = "Sample size (n)", ylab = "MSE", main = "MSE of beta hat")
lines(n_grid, mom_mse_beta, type = "b", col = "red", pch = 17)
legend("topright", legend = c("MLE", "MoM"), col = c("blue", "red"),
       pch = c(16, 17), lty = 1, cex = 0.8)
```

Both MLE and MoM are positively biased for $\alpha$ and negatively biased for $\beta$ especially at small $n$. As $n$ increases, both biases shrink toward $0$. This confirms both estimators are consistent. MLE has slightly larger bias for $\alpha$ at $n=20$ ($0.55$ vs.\ $0.49$), but the difference is small and disappears quickly. For MSE, it decreases as $n$ grows for both methods, again confirming consistency. Also, MLE has smaller MSE than MoM at every sample size.
$$
    \mathrm{MLE\_MSE}_{\alpha}=1.94 \quad \text{vs.} \quad \mathrm{MoM\_MSE}_{\alpha}=2.00,
$$
  and the gap is more visible for $\beta$:
$$ 
    \mathrm{MLE\_MSE}_{\beta}=5.42 \quad \text{vs.} \quad \mathrm{MoM\_MSE}_{\beta}=7.05.
$$
This means MLE is more efficient, it gives more precise estimates on average.

The reason why MLE is better than MoM in this case, MLE uses the full likelihood, while MoM only matches mean and variance. MLE is asymptotically efficient, meaning it achieves the lowest possible MSE among consistent estimators as $n \to \infty$. MoM does not have this property.

Both methods work and improve with larger samples, but MLE is the better choice --- it has lower MSE, meaning more accurate estimates overall. The advantage is most noticeable at small sample sizes and for the $\beta$ (scale) parameter.



## problem 4

We generate random covariance matrices: simulate an $m \times m$ matrix $R$ with i.i.d. $N(0,1)$ entries, then form $\Sigma = RR^T$. This gives a Wishart distribution $W_m(I_m, m)$.


### Part (1): Expected Value of trace($\Sigma$)

**Analytical derivation:**

$$\text{trace}(\Sigma) = \text{trace}(RR^T) = \sum_{i=1}^{m} \sum_{j=1}^{m} R_{ij}^2$$

Each $R_{ij} \sim N(0,1)$, so $R_{ij}^2 \sim \chi^2(1)$ and $E[R_{ij}^2] = 1$.

There are $m^2$ such terms, so:

$$E[\text{trace}(\Sigma)] = m^2$$

For $m = 100$: $E[\text{trace}] = 10{,}000$. For $m = 1000$: $E[\text{trace}] = 1{,}000{,}000$.

**Simulation for m = 100:**

```{r part1_m100}
set.seed(42)
m <- 100
B <- 500  # number of Monte Carlo replicates

trace_vals <- numeric(B)
for (i in 1:B) {
  R <- matrix(rnorm(m * m), nrow = m, ncol = m)
  # trace(RR^T) = sum of all squared entries
  trace_vals[i] <- sum(R^2)
}

mc_mean <- mean(trace_vals)
mc_se   <- sd(trace_vals) / sqrt(B)

cat("m = 100\n")
cat("Analytical E[trace] =", m^2, "\n")
cat("MC estimate         =", round(mc_mean, 2), "\n")
cat("MC standard error   =", round(mc_se, 2), "\n")
cat("Sample size B       =", B, "\n")

plot(density(trace_vals), main = "Density of trace(Sigma), m = 100",
     xlab = "trace(Sigma)", col = "blue", lwd = 2)
abline(v = m^2, col = "red", lty = 2, lwd = 2)
legend("topright", legend = c("MC density", "True E[trace]"),
       col = c("blue", "red"), lty = c(1, 2), lwd = 2, cex = 0.8)
```

**Simulation for m = 1000:**

```{r part1_m1000}
set.seed(42)
m <- 1000
B <- 100  # fewer reps since m=1000 is expensive

trace_vals <- numeric(B)
for (i in 1:B) {
  R <- matrix(rnorm(m * m), nrow = m, ncol = m)
  trace_vals[i] <- sum(R^2)
}

mc_mean <- mean(trace_vals)
mc_se   <- sd(trace_vals) / sqrt(B)

cat("m = 1000\n")
cat("Analytical E[trace] =", m^2, "\n")
cat("MC estimate         =", round(mc_mean, 2), "\n")
cat("MC standard error   =", round(mc_se, 2), "\n")
cat("Sample size B       =", B, "\n")

plot(density(trace_vals), main = "Density of trace(Sigma), m = 1000",
     xlab = "trace(Sigma)", col = "blue", lwd = 2)
abline(v = m^2, col = "red", lty = 2, lwd = 2)
legend("topright", legend = c("MC density", "True E[trace]"),
       col = c("blue", "red"), lty = c(1, 2), lwd = 2, cex = 0.8)
```
The Monte Carlo results closely match the analytical expectation $\mathbb{E}\{\mathrm{tr}(\Sigma)\}=m^2$.
For $m=100$, the MC estimate is $10000.41$ (true value $=10000$) with MC standard error $6.12$ using $B=500$ replicates.
For $m=1000$, the MC estimate is $1000132$ (true value $=10^6$) with MC standard error $163.1$ using $B=100$ replicates.
In both cases, the true expectation (red dashed line) lies near the center of the estimated density, indicating negligible Monte Carlo bias.

The density is approximately bell-shaped, which is consistent with the fact that
$$
\mathrm{tr}(\Sigma)=\sum_{i=1}^m\sum_{j=1}^m R_{ij}^2 \sim \chi^2_{m^2},
$$
and for large degrees of freedom a chi-square distribution is well-approximated by a normal distribution (CLT).
Moreover, although the mean grows as $m^2$, the distribution becomes relatively more concentrated as $m$ increases: the relative MC standard errors are $\frac{6.12}{10000}\approx 6.1\times 10^{-4}$,$\frac{163.1}{10^6}\approx 1.6\times 10^{-4}$ showing higher precision (in relative terms) for larger $m$.

### Part (2): Largest Eigenvalue of $\Sigma$

For the largest eigenvalue, there is no simple closed-form expectation. By the Tracy-Widom law, for a Wishart matrix $W_m(I, m)$, the largest eigenvalue is approximately $(\sqrt{m} + \sqrt{m})^2 = 4m$ (using the Marchenko-Pastur result when $n = m$, so the ratio $p/n = 1$, giving upper edge at $(\sqrt{1}+1)^2 \cdot m = 4m$).

We approximate it by simulation.

**Simulation for m = 100:**

```{r part2_m100}
set.seed(42)
m <- 100
B <- 200

max_eig <- numeric(B)
for (i in 1:B) {
  R <- matrix(rnorm(m * m), nrow = m, ncol = m)
  Sigma <- R %*% t(R)
  # Only need the largest eigenvalue; use symmetric=TRUE for speed
  max_eig[i] <- max(eigen(Sigma, symmetric = TRUE, only.values = TRUE)$values)
}

mc_mean <- mean(max_eig)
mc_se   <- sd(max_eig) / sqrt(B)

cat("m = 100\n")
cat("MC estimate of E[lambda_1] =", round(mc_mean, 2), "\n")
cat("MC standard error           =", round(mc_se, 2), "\n")
cat("Sample size B               =", B, "\n")

plot(density(max_eig), main = "Density of Largest Eigenvalue, m = 100",
     xlab = "Largest eigenvalue", col = "blue", lwd = 2)
abline(v = mc_mean, col = "red", lty = 2, lwd = 2)
legend("topright", legend = c("MC density", "MC mean"),
       col = c("blue", "red"), lty = c(1, 2), lwd = 2, cex = 0.8)
```

**Simulation for m = 1000:**

```{r part2_m1000}
set.seed(42)
m <- 1000
B <- 20  # small B because eigendecomposition of 1000x1000 is expensive

max_eig <- numeric(B)
for (i in 1:B) {
  R <- matrix(rnorm(m * m), nrow = m, ncol = m)
  Sigma <- R %*% t(R)
  max_eig[i] <- max(eigen(Sigma, symmetric = TRUE, only.values = TRUE)$values)
}

mc_mean <- mean(max_eig)
mc_se   <- sd(max_eig) / sqrt(B)

cat("m = 1000\n")
cat("MC estimate of E[lambda_1] =", round(mc_mean, 2), "\n")
cat("MC standard error           =", round(mc_se, 2), "\n")
cat("Sample size B               =", B, "\n")

plot(density(max_eig), main = "Density of Largest Eigenvalue, m = 1000",
     xlab = "Largest eigenvalue", col = "blue", lwd = 2)
abline(v = mc_mean, col = "red", lty = 2, lwd = 2)
legend("topright", legend = c("MC density", "MC mean"),
       col = c("blue", "red"), lty = c(1, 2), lwd = 2, cex = 0.8)
```

There is no simple closed-form expression for $\mathbb{E}(\lambda_1)$, so we approximate it via Monte Carlo simulation.
For $m=100$, the Monte Carlo estimate is $\widehat{\mathbb{E}}(\lambda_1)=384.71$ with MC standard error $1.05$ using $B=200$ replicates.
For $m=1000$, the estimate is $\widehat{\mathbb{E}}(\lambda_1)=3973.66$ with MC standard error $7.53$ using $B=20$ replicates.

The density plots show that the distribution of $\lambda_1$ is unimodal and slightly right-skewed, reflecting that the largest eigenvalue is an extreme-value-type statistic and can occasionally take unusually large values.
As $m$ increases from $100$ to $1000$, the typical size of $\lambda_1$ increases substantially (from around $3.85\times 10^2$ to about $3.97\times 10^3$), consistent with random matrix theory intuition that the top eigenvalue grows on the order of $m$ for Wishart matrices.

As a rough theoretical benchmark, for $\Sigma \sim W_m(I_m,m)$ (i.e., $n=m$ and population covariance $I_m$), the Marchenko--Pastur upper spectral edge suggests $\lambda_1$ should be close to $(\sqrt{m}+\sqrt{m})^2 = 4m.$
Indeed, $4m=400$ when $m=100$ and $4m=4000$ when $m=1000$, which are close to the Monte Carlo means ($384.71$ and $3973.66$). This agreement supports that our simulation is producing reasonable values for the largest eigenvalue and that $\mathbb{E}(\lambda_1)$ scales approximately linearly with $m$.


### Part (3): E(lambda_1) as a function of m

#### (a) & (b): Plot and grid of m values

```{r part3}
set.seed(42)
m_grid <- c(10, 25, 50, 100, 200, 500, 1000)
B_vec  <- c(200, 200, 200, 100, 50, 20, 10)  # fewer reps for larger m

mean_max_eig <- numeric(length(m_grid))
se_max_eig   <- numeric(length(m_grid))

for (k in seq_along(m_grid)) {
  m <- m_grid[k]
  B <- B_vec[k]
  eigs <- numeric(B)
  
  for (i in 1:B) {
    R <- matrix(rnorm(m * m), nrow = m, ncol = m)
    Sigma <- R %*% t(R)
    eigs[i] <- max(eigen(Sigma, symmetric = TRUE, only.values = TRUE)$values)
  }
  
  mean_max_eig[k] <- mean(eigs)
  se_max_eig[k]   <- sd(eigs) / sqrt(B)
}

# Results table
results <- data.frame(
  m = m_grid,
  B = B_vec,
  E_lambda1 = round(mean_max_eig, 2),
  MC_SE     = round(se_max_eig, 2)
)
print(results)

# Plot
plot(m_grid, mean_max_eig, type = "b", col = "blue", pch = 16, lwd = 2,
     xlab = "m (matrix dimension)", ylab = "Approximate E(lambda_1)",
     main = "Expected Largest Eigenvalue vs m")

# Add theoretical reference line: E[lambda_1] â‰ˆ 4m for Wishart(I, m)
lines(m_grid, 4 * m_grid, col = "red", lty = 2, lwd = 2)
legend("topleft", legend = c("MC estimate", "4m reference"),
       col = c("blue", "red"), lty = c(1, 2), pch = c(16, NA), lwd = 2, cex = 0.8)
```



From the Monte Carlo table, we estimated $E(\lambda_1)$ on the grid  
$m \in \{10, 25, 50, 100, 200, 500, 1000\}$, using fewer replicates $B$ for larger $m$ to keep computation feasible (from $B=200$ at small $m$ down to $B=10$ at $m=1000$).

The estimated $E(\lambda_1)$ increases monotonically with $m$ and is approximately linear. For example, it rises from $32.28$ at $m=10$ to $3965.72$ at $m=1000$. The plot shows the Monte Carlo curve matches the theoretical reference line $4m$ very closely, which is consistent with the Marchenko--Pastur upper-edge heuristic for $\Sigma \sim W_m(I_m, m)$, where $\lambda_1$ is expected to be near $(\sqrt{m}+\sqrt{m})^2 = 4m$.

Across all $m$, the estimates are slightly below $4m$ (e.g., $3965.72$ vs.\ $4000$ when $m=1000$), which is reasonable for finite $m$ due to random fluctuations of the extreme eigenvalue.

Monte Carlo standard errors increase in absolute size for larger $m$ mainly because we used smaller $B$. However, the relative Monte Carlo error remains small; for $m=1000$, $12.73/3965.72 \approx 0.0032$, so the linear trend and the agreement with the $4m$ benchmark are still clearly supported.

**Largest $m$ used:** The largest dimension considered here is $m=1000$. The main limitation is computational cost, since forming $\Sigma = RR^T$ and computing the largest eigenvalue both scale on the order of $m^3$ per replicate.

#### (c) Largest m value

```{r part3c}
cat("Largest m value used:", max(m_grid), "\n")
```

The largest $m$ for which we approximated the expectation is $m = 1000$. The limiting factor is computation time: forming $\Sigma = RR^T$ costs $O(m^3)$ flops, and eigendecomposition also costs $O(m^3)$.



### Part (4): Computational Complexity

For a single Monte Carlo replicate at dimension $m$:

1. **Generate R**: $m^2$ random normal draws $\rightarrow O(m^2)$
2. **Compute $\Sigma = RR^T$**: Matrix multiplication of $m \times m$ by $m \times m$ $\rightarrow O(m^3)$ flops (specifically about $2m^3$ flops)
3. **Eigendecomposition of $\Sigma$**: $O(m^3)$ flops

So **one replicate** costs $O(m^3)$ flops.

Over $B$ Monte Carlo replicates, the **total cost** is:

$$O(B \cdot m^3) \text{ flops}$$

If we do this for multiple $m$ values on a grid $\{m_1, m_2, \ldots, m_K\}$, the total cost is:

$$\sum_{k=1}^{K} B_k \cdot O(m_k^3)$$

The dominant cost comes from the largest $m$ value. For $m = 1000$ with $B = 10$ replicates, that is roughly $10 \times 2 \times (1000)^3 = 2 \times 10^{10}$ flops.
























